# ‚úÖ scraper.py ‚Äî –º–æ–¥—É–ª—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π

import requests
from bs4 import BeautifulSoup
from typing import List
from datetime import date

# –¢–∏–ø –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏
from app.schemas.vacancy import VacancyCreate

# üîß –ü—Ä–æ—Å—Ç–µ–π—à–∏–π —à–∞–±–ª–æ–Ω –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
def parse_jobs_from_url(url: str, company_id: int) -> List[VacancyCreate]:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {url}: {e}")
        return []

    soup = BeautifulSoup(response.text, 'html.parser')
    vacancies = []

    # üîç –≠—Ç–æ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    job_elements = soup.select('div.job, li.job, .vacancy')  # CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä—ã

    for job in job_elements:
        title = job.find('h2') or job.find('a')
        location = job.find(class_='location') or job.find('span', string="Location")

        if not title:
            continue

        vacancy = VacancyCreate(
            title=title.get_text(strip=True),
            location=location.get_text(strip=True) if location else "",
            link=url,
            apply_link=url,  # üí° –µ—Å–ª–∏ –Ω–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–∏, —Å—Ç–∞–≤–∏–º —Ç—É –∂–µ
            published_at=date.today(),
            email="",  # –µ—Å–ª–∏ –µ—Å—Ç—å, –≤—ã—Ç–∞—â–∏–º –ø–æ–∑–∂–µ
            remote="remote" in title.get_text(strip=True).lower(),
            source="website",
            description="",  # –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ–ª–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞ ‚Äî –¥–æ–±–∞–≤–∏–º
            trust_score=50,  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            relevance_score=50,
            company_id=company_id
        )
        vacancies.append(vacancy)

    print(f"‚úÖ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ {url}")
    return vacancies
